{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dMCBup9_vfG"
      },
      "source": [
        "# CIS421 Fall 2023\n",
        "Final Project - classification with CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EQcZIVk_vfO"
      },
      "source": [
        "In this notebook we implement the approched described in this [paper](https://arxiv.org/pdf/1408.5882.pdf) for classifiying sentences using Convolutional Neural Networks. In particular, we will classify sentences into \"subjective\" or \"objective\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZd9Coeq_vfP"
      },
      "source": [
        "## Subjectivity Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBYe4vuf_vfP"
      },
      "source": [
        "The subjectivity dataset has 5000 subjective and 5000 objective processed sentences. To get the data, write a function\n",
        "\n",
        "```python\n",
        "def unpack_dataset()\n",
        "```\n",
        "that\n",
        "\n",
        "- downloads from https://cis335.guihang.org/data/rotten_imdb.tar.gz\n",
        "- makes a folder `data`\n",
        "- unpacks the package into `data` folder with bash command:\n",
        "\n",
        "```bash\n",
        " tar -xvf rotten_imdb.tar.gz -C data\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtMI1YcTNjs4"
      },
      "source": [
        "## <font color=\"red\">Your code here:</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPplBB-YCd_j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "import tarfile\n",
        "\n",
        "def unpack_dataset():\n",
        "  # you code here\n",
        "  newpath = r'/content/data'\n",
        "  if not os.path.exists(newpath):\n",
        "      os.makedirs(newpath)\n",
        "\n",
        "  url = ('https://cis335.guihang.org/data/rotten_imdb.tar.gz')\n",
        "  filename = os.path.join(newpath, 'rotten_imdb.tar.gz')\n",
        "  urlretrieve(url, filename)\n",
        "\n",
        "  # os.system('tar -xvf /content/data/rotten_imdb.tar.gz -C /content/data')\n",
        "  with tarfile.open(filename, \"r\") as tf:\n",
        "      tf.extractall(path=newpath)\n",
        "      print(\"All files extracted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj6jFwGm_vfR",
        "outputId": "26d0544c-1e3b-44f2-97f2-f795522e28f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All files extracted\n"
          ]
        }
      ],
      "source": [
        "unpack_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko4cBd5-_vfU",
        "outputId": "0e669f9c-5797-495e-b83b-73b0f58df005"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PosixPath('data/glove.6B.200d.txt'),\n",
              " PosixPath('data/glove.6B.50d.txt'),\n",
              " PosixPath('data/rotten_imdb.tar.gz'),\n",
              " PosixPath('data/quote.tok.gt9.5000'),\n",
              " PosixPath('data/glove.6B.100d.txt'),\n",
              " PosixPath('data/subjdata.README.1.0'),\n",
              " PosixPath('data/plot.tok.gt9.5000'),\n",
              " PosixPath('data/glove.6B.300d.txt')]"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "PATH = Path(\"data\")\n",
        "list(PATH.iterdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uxJfQpI_vfV"
      },
      "source": [
        "Read `subjdata.README.1.0` file:\n",
        "- we have one file containing 5000 subjective sentences (or snippets)\n",
        "- another file contains 5000 objective sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwgmwemF_vfW",
        "outputId": "c277d685-509b-4318-9b57-58c60b34b7a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the movie begins in the past where a young boy named sam attempts to save celebi from a hunter . \n",
            "emerging from the human psyche and showing characteristics of abstract expressionism , minimalism and russian constructivism , graffiti removal has secured its place in the history of modern art while being created by artists who are unconscious of their artistic achievements . \n",
            "spurning her mother's insistence that she get on with her life , mary is thrown out of the house , rejected by joe , and expelled from school as she grows larger with child . \n",
            "amitabh can't believe the board of directors and his mind is filled with revenge and what better revenge than robbing the bank himself , ironic as it may sound . \n",
            "she , among others excentricities , talks to a small rock , gertrude , like if she was alive . \n",
            "this gives the girls a fair chance of pulling the wool over their eyes using their sexiness to poach any last vestige of common sense the dons might have had . \n",
            "styled after vh1's \" behind the music , \" this mockumentary profiles the rise and fall of an internet startup , called icevan . com . \n",
            "being blue is not his only predicament ; he also lacks the ability to outwardly express his emotions . \n",
            "the killer's clues are a perversion of biblical punishments for sins : stoning , burning , decapitation . \n",
            "david is a painter with painter's block who takes a job as a waiter to get some inspiration . \n"
          ]
        }
      ],
      "source": [
        "! head data/plot.tok.gt9.5000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg3fqelm_vfX"
      },
      "source": [
        "## String cleaning functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsfWyDCj_vfY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rynMEwATCkNY"
      },
      "source": [
        "## <font color=\"red\">Your code here:</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP8FlsFp_vfY"
      },
      "outputs": [],
      "source": [
        "def read_file(inputFile, encoding='utf-8'):\n",
        "    \"\"\" Read file returns a numpy list.\n",
        "    \"\"\"\n",
        "    # Your code here: read file and split into lines\n",
        "    with open(inputFile, 'r', encoding=encoding) as read_file:\n",
        "      content = read_file.readlines()\n",
        "\n",
        "    content = np.array(content)\n",
        "    return content # content is a 1-D numpy (equivalent to list) of text lines read from input inputFile:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh9fa9eE_vfZ"
      },
      "outputs": [],
      "source": [
        "def get_vocab(list_of_doc):\n",
        "    \"\"\"\n",
        "    Input: a list of documents. Each list item is a document.\n",
        "\n",
        "    Computes Dictionary of counts of words.\n",
        "    Dict keys: each individual word in all docs in the input list\n",
        "    Dict values: how many documents contains each of the word in keys? Use that count as a value for that key(word)\n",
        "    Returns Dict\n",
        "    \"\"\"\n",
        "    # your code\n",
        "    vocab = {}\n",
        "    for doc in list_of_doc:\n",
        "      doc_words = doc.split()\n",
        "      for i in doc_words:\n",
        "        vocab[i] = vocab.get(i, 0) + 1\n",
        "    return vocab # vocab is a dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LyDrdP_5zyoV",
        "outputId": "212464c3-7ef8-4a36-cc5b-df7f8738303e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['the movie begins in the past where a young boy named sam attempts to save celebi from a hunter . \\n',\n",
              "       'emerging from the human psyche and showing characteristics of abstract expressionism , minimalism and russian constructivism , graffiti removal has secured its place in the history of modern art while being created by artists who are unconscious of their artistic achievements . \\n',\n",
              "       \"spurning her mother's insistence that she get on with her life , mary is thrown out of the house , rejected by joe , and expelled from school as she grows larger with child . \\n\",\n",
              "       ...,\n",
              "       'enter the beautiful and mysterious secret agent petra schmitt . \\n',\n",
              "       'after listening to a missionary from china speak , a christian man ( josh gaffga ) becomes very convinced by what he hears . \\n',\n",
              "       'looking for a short cut to fame , glass concocted sources , quotes and even entire stories , but his deception did not go unnoticed forever , and eventually , his world came crumbling down . . . \\n'],\n",
              "      dtype='<U693')"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp = read_file('/content/data/plot.tok.gt9.5000')\n",
        "temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H1JYTZ1k-pUk",
        "outputId": "e1a3a4e0-88b6-4201-ff82-93f6152fcdac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'the': 6311,\n",
              " 'movie': 60,\n",
              " 'begins': 79,\n",
              " 'in': 2144,\n",
              " 'past': 71,\n",
              " 'where': 169,\n",
              " 'a': 4106,\n",
              " 'young': 250,\n",
              " 'boy': 72,\n",
              " 'named': 64,\n",
              " 'sam': 33,\n",
              " 'attempts': 28,\n",
              " 'to': 3307,\n",
              " 'save': 55,\n",
              " 'celebi': 6,\n",
              " 'from': 516,\n",
              " 'hunter': 16,\n",
              " '.': 5387,\n",
              " 'emerging': 1,\n",
              " 'human': 33,\n",
              " 'psyche': 3,\n",
              " 'and': 3571,\n",
              " 'showing': 7,\n",
              " 'characteristics': 3,\n",
              " 'of': 3117,\n",
              " 'abstract': 1,\n",
              " 'expressionism': 1,\n",
              " ',': 6554,\n",
              " 'minimalism': 1,\n",
              " 'russian': 14,\n",
              " 'constructivism': 1,\n",
              " 'graffiti': 4,\n",
              " 'removal': 3,\n",
              " 'has': 515,\n",
              " 'secured': 1,\n",
              " 'its': 73,\n",
              " 'place': 63,\n",
              " 'history': 34,\n",
              " 'modern': 24,\n",
              " 'art': 31,\n",
              " 'while': 146,\n",
              " 'being': 104,\n",
              " 'created': 18,\n",
              " 'by': 581,\n",
              " 'artists': 9,\n",
              " 'who': 714,\n",
              " 'are': 440,\n",
              " 'unconscious': 3,\n",
              " 'their': 646,\n",
              " 'artistic': 11,\n",
              " 'achievements': 1,\n",
              " 'spurning': 1,\n",
              " 'her': 990,\n",
              " \"mother's\": 16,\n",
              " 'insistence': 2,\n",
              " 'that': 852,\n",
              " 'she': 453,\n",
              " 'get': 162,\n",
              " 'on': 806,\n",
              " 'with': 1108,\n",
              " 'life': 362,\n",
              " 'mary': 15,\n",
              " 'is': 1756,\n",
              " 'thrown': 11,\n",
              " 'out': 352,\n",
              " 'house': 70,\n",
              " 'rejected': 5,\n",
              " 'joe': 23,\n",
              " 'expelled': 4,\n",
              " 'school': 109,\n",
              " 'as': 710,\n",
              " 'grows': 9,\n",
              " 'larger': 3,\n",
              " 'child': 36,\n",
              " 'amitabh': 6,\n",
              " \"can't\": 39,\n",
              " 'believe': 12,\n",
              " 'board': 11,\n",
              " 'directors': 2,\n",
              " 'his': 1656,\n",
              " 'mind': 24,\n",
              " 'filled': 14,\n",
              " 'revenge': 30,\n",
              " 'what': 176,\n",
              " 'better': 28,\n",
              " 'than': 91,\n",
              " 'robbing': 6,\n",
              " 'bank': 37,\n",
              " 'himself': 113,\n",
              " 'ironic': 1,\n",
              " 'it': 397,\n",
              " 'may': 58,\n",
              " 'sound': 6,\n",
              " 'among': 21,\n",
              " 'others': 21,\n",
              " 'excentricities': 1,\n",
              " 'talks': 10,\n",
              " 'small': 59,\n",
              " 'rock': 25,\n",
              " 'gertrude': 1,\n",
              " 'like': 71,\n",
              " 'if': 77,\n",
              " 'was': 157,\n",
              " 'alive': 16,\n",
              " 'this': 358,\n",
              " 'gives': 23,\n",
              " 'girls': 36,\n",
              " 'fair': 3,\n",
              " 'chance': 41,\n",
              " 'pulling': 2,\n",
              " 'wool': 1,\n",
              " 'over': 104,\n",
              " 'eyes': 24,\n",
              " 'using': 34,\n",
              " 'sexiness': 1,\n",
              " 'poach': 1,\n",
              " 'any': 39,\n",
              " 'last': 61,\n",
              " 'vestige': 1,\n",
              " 'common': 13,\n",
              " 'sense': 16,\n",
              " 'dons': 5,\n",
              " 'might': 31,\n",
              " 'have': 218,\n",
              " 'had': 73,\n",
              " 'styled': 1,\n",
              " 'after': 239,\n",
              " \"vh1's\": 1,\n",
              " '\"': 739,\n",
              " 'behind': 46,\n",
              " 'music': 35,\n",
              " 'mockumentary': 2,\n",
              " 'profiles': 1,\n",
              " 'rise': 12,\n",
              " 'fall': 31,\n",
              " 'an': 667,\n",
              " 'internet': 16,\n",
              " 'startup': 2,\n",
              " 'called': 38,\n",
              " 'icevan': 1,\n",
              " 'com': 6,\n",
              " 'blue': 11,\n",
              " 'not': 210,\n",
              " 'only': 194,\n",
              " 'predicament': 4,\n",
              " ';': 266,\n",
              " 'he': 1006,\n",
              " 'also': 103,\n",
              " 'lacks': 2,\n",
              " 'ability': 12,\n",
              " 'outwardly': 2,\n",
              " 'express': 2,\n",
              " 'emotions': 8,\n",
              " \"killer's\": 1,\n",
              " 'clues': 9,\n",
              " 'perversion': 1,\n",
              " 'biblical': 1,\n",
              " 'punishments': 1,\n",
              " 'for': 860,\n",
              " 'sins': 5,\n",
              " ':': 134,\n",
              " 'stoning': 1,\n",
              " 'burning': 8,\n",
              " 'decapitation': 1,\n",
              " 'david': 25,\n",
              " 'painter': 5,\n",
              " \"painter's\": 2,\n",
              " 'block': 11,\n",
              " 'takes': 88,\n",
              " 'job': 73,\n",
              " 'waiter': 1,\n",
              " 'some': 119,\n",
              " 'inspiration': 2,\n",
              " 'women': 70,\n",
              " 'craved': 1,\n",
              " 'him': 396,\n",
              " 'men': 56,\n",
              " 'wanted': 14,\n",
              " 'be': 312,\n",
              " 'set': 94,\n",
              " 'island': 25,\n",
              " 'off': 93,\n",
              " 'coast': 9,\n",
              " 'florida': 7,\n",
              " 'techno': 2,\n",
              " 'rave': 2,\n",
              " 'party': 25,\n",
              " 'attracts': 2,\n",
              " 'diverse': 5,\n",
              " 'group': 65,\n",
              " 'college': 32,\n",
              " 'coeds': 1,\n",
              " 'guard': 13,\n",
              " 'officer': 12,\n",
              " 'lesson': 7,\n",
              " 'learned': 6,\n",
              " 'never': 71,\n",
              " 'mess': 5,\n",
              " 'gay': 28,\n",
              " 'mafia': 14,\n",
              " '!': 46,\n",
              " 'theme': 3,\n",
              " 'film': 194,\n",
              " 'simultaneously': 6,\n",
              " 'addresses': 1,\n",
              " 'similarities': 3,\n",
              " 'between': 111,\n",
              " 'two': 219,\n",
              " 'factions': 2,\n",
              " 'law': 21,\n",
              " 'crime': 36,\n",
              " 'revealing': 12,\n",
              " 'brothers': 19,\n",
              " \"they're\": 22,\n",
              " 'jewish': 10,\n",
              " 'grandmothers': 1,\n",
              " 'lesbians': 1,\n",
              " 'but': 473,\n",
              " \"he's\": 94,\n",
              " 'neglecting': 1,\n",
              " 'work': 82,\n",
              " 'carping': 1,\n",
              " 'at': 371,\n",
              " 'mom': 13,\n",
              " 'behaving': 1,\n",
              " 'badly': 4,\n",
              " 'toward': 8,\n",
              " 'loyal': 5,\n",
              " 'friend': 83,\n",
              " 'bobbi': 1,\n",
              " 'all': 262,\n",
              " 'going': 48,\n",
              " \"gerry's\": 2,\n",
              " 'estranged': 9,\n",
              " 'wife': 83,\n",
              " 'margaret': 1,\n",
              " 'worried': 4,\n",
              " \"daughter's\": 4,\n",
              " 'safety': 5,\n",
              " 'finds': 134,\n",
              " 'herself': 54,\n",
              " 'another': 64,\n",
              " 'target': 6,\n",
              " 'race': 23,\n",
              " 'find': 178,\n",
              " 'code': 13,\n",
              " 'valento': 3,\n",
              " 'feels': 11,\n",
              " 'heat': 8,\n",
              " 'turns': 60,\n",
              " 'table': 3,\n",
              " 'makes': 50,\n",
              " 'dupe': 4,\n",
              " 'into': 316,\n",
              " 'one': 346,\n",
              " 'own': 153,\n",
              " 'rubs': 1,\n",
              " \"da's\": 1,\n",
              " 'nose': 1,\n",
              " 'saigon': 2,\n",
              " '1952': 1,\n",
              " 'beautiful': 58,\n",
              " 'exotic': 8,\n",
              " 'mysterious': 59,\n",
              " 'city': 83,\n",
              " 'caught': 22,\n",
              " 'grips': 2,\n",
              " 'vietnamese': 5,\n",
              " 'war': 76,\n",
              " 'liberation': 2,\n",
              " 'french': 16,\n",
              " 'colonial': 3,\n",
              " 'powers': 19,\n",
              " 'deep': 18,\n",
              " 'northwest': 4,\n",
              " 'there': 93,\n",
              " 'lone': 1,\n",
              " 'ranch': 2,\n",
              " 'tucked': 2,\n",
              " 'away': 70,\n",
              " 'so': 101,\n",
              " 'purposefully': 2,\n",
              " 'way': 139,\n",
              " 'looking': 40,\n",
              " 'teenager': 9,\n",
              " 'father': 135,\n",
              " \"there's\": 19,\n",
              " 'silver': 6,\n",
              " 'lead': 35,\n",
              " 'says': 8,\n",
              " 'rikki': 2,\n",
              " 'ortega': 1,\n",
              " 'moves': 24,\n",
              " 'king': 27,\n",
              " 'street': 35,\n",
              " '&#193': 3,\n",
              " 'nglio': 1,\n",
              " 'l': 15,\n",
              " \"'s\": 7,\n",
              " 'east': 17,\n",
              " 'side': 26,\n",
              " 'these': 68,\n",
              " 'games': 6,\n",
              " 'chasing': 7,\n",
              " 'rejecting': 1,\n",
              " 'seducing': 4,\n",
              " 'played': 33,\n",
              " 'economically': 2,\n",
              " 'spiritually': 3,\n",
              " 'depressed': 6,\n",
              " 'hong': 10,\n",
              " 'kong': 7,\n",
              " 'without': 34,\n",
              " 'much': 49,\n",
              " 'gusto': 1,\n",
              " 'television': 17,\n",
              " 'made': 45,\n",
              " 'famous': 23,\n",
              " 'biggest': 14,\n",
              " 'hits': 8,\n",
              " 'happened': 11,\n",
              " 'screen': 6,\n",
              " 'jordan': 10,\n",
              " 'long': 58,\n",
              " 'search': 53,\n",
              " 'true': 84,\n",
              " 'faith': 11,\n",
              " 'tries': 55,\n",
              " 'protect': 19,\n",
              " 'believes': 12,\n",
              " 'injustice': 5,\n",
              " \"'bloody\": 1,\n",
              " \"magic'\": 1,\n",
              " 'story': 276,\n",
              " 'zack': 2,\n",
              " 'eleven': 3,\n",
              " 'year': 86,\n",
              " 'old': 142,\n",
              " \"who's\": 22,\n",
              " 'family': 169,\n",
              " 'visited': 3,\n",
              " 'three': 97,\n",
              " 'debt': 8,\n",
              " 'collectors': 2,\n",
              " 'however': 77,\n",
              " 'jane': 7,\n",
              " \"wendy's\": 1,\n",
              " '12-year-old': 4,\n",
              " 'daughter': 86,\n",
              " 'sees': 32,\n",
              " 'make': 105,\n",
              " 'refuses': 10,\n",
              " 'tales': 4,\n",
              " 'elegant': 1,\n",
              " 'documentary': 49,\n",
              " 'sundance': 1,\n",
              " 'eloquent': 1,\n",
              " 'deeply': 10,\n",
              " 'moving': 11,\n",
              " 'la': 17,\n",
              " 'times': 18,\n",
              " 'toyo': 1,\n",
              " 'miyatake': 2,\n",
              " 'infinite': 2,\n",
              " 'shades': 1,\n",
              " 'gray': 4,\n",
              " 'penetrating': 1,\n",
              " 'portrait': 10,\n",
              " \"photographer's\": 1,\n",
              " 'truth': 30,\n",
              " 'beauty': 16,\n",
              " 'world': 236,\n",
              " 'impermanence': 1,\n",
              " 'straight': 7,\n",
              " 'up': 263,\n",
              " 'helicopters': 1,\n",
              " 'action': 33,\n",
              " 'will': 204,\n",
              " 'take': 98,\n",
              " 'audiences': 3,\n",
              " 'series': 33,\n",
              " 'aerial': 1,\n",
              " 'adventures': 8,\n",
              " 'lapp': 1,\n",
              " 'woman': 117,\n",
              " 'anni': 2,\n",
              " 'shelter': 13,\n",
              " 'both': 76,\n",
              " 'them': 209,\n",
              " 'farm': 14,\n",
              " 'touches': 2,\n",
              " 'encroachment': 1,\n",
              " 'christianity': 1,\n",
              " 'brought': 19,\n",
              " 'missionaries': 2,\n",
              " 'which': 153,\n",
              " 'odds': 6,\n",
              " \"mepe's\": 2,\n",
              " 'tribal': 5,\n",
              " 'traditional': 13,\n",
              " 'roots': 8,\n",
              " 'grisly': 3,\n",
              " 'murders': 17,\n",
              " 'brings': 27,\n",
              " 'fbi': 18,\n",
              " 'agent': 34,\n",
              " 'graham': 17,\n",
              " '(': 778,\n",
              " 'norton': 2,\n",
              " ')': 778,\n",
              " 'retirement': 6,\n",
              " 'puts': 15,\n",
              " 'atrocious': 1,\n",
              " 'killer': 32,\n",
              " 'fiennes': 1,\n",
              " 'driven': 8,\n",
              " 'image': 4,\n",
              " 'painting': 2,\n",
              " 'soon': 101,\n",
              " 'team': 50,\n",
              " 'suspect': 12,\n",
              " \"knowles'\": 1,\n",
              " 'main': 14,\n",
              " 'objective': 2,\n",
              " 'actually': 22,\n",
              " 'recover': 8,\n",
              " 'prototype': 4,\n",
              " 'dna': 8,\n",
              " 'testing': 7,\n",
              " 'machine': 14,\n",
              " 'huxley': 2,\n",
              " 'project': 16,\n",
              " 'company': 24,\n",
              " 'spent': 9,\n",
              " 'years': 161,\n",
              " 'millions': 6,\n",
              " 'dollars': 10,\n",
              " 'developing': 2,\n",
              " 'mother': 112,\n",
              " 'persuades': 3,\n",
              " 'renowned': 4,\n",
              " 'entomologist': 1,\n",
              " 'trip': 36,\n",
              " 'jungle': 5,\n",
              " 'butterfly': 5,\n",
              " 'leading': 15,\n",
              " 'adventure': 17,\n",
              " 'transform': 4,\n",
              " 'lives': 123,\n",
              " 'rare': 9,\n",
              " 'gift': 10,\n",
              " 'melding': 1,\n",
              " 'subjectivity': 1,\n",
              " 'biographical': 2,\n",
              " 'facts': 8,\n",
              " 'm&#225': 4,\n",
              " 'rton': 1,\n",
              " 'sabina': 1,\n",
              " 'spielrein': 3,\n",
              " 'back': 158,\n",
              " 'body': 29,\n",
              " 'soul': 16,\n",
              " 'seeking': 10,\n",
              " 'mental': 10,\n",
              " 'escape': 45,\n",
              " 'simone': 4,\n",
              " 'tune': 1,\n",
              " \"what's\": 5,\n",
              " 'happening': 7,\n",
              " 'other': 138,\n",
              " 'couples': 8,\n",
              " 'around': 61,\n",
              " 'beatle': 1,\n",
              " 'fan': 8,\n",
              " 'drama': 25,\n",
              " 'about': 264,\n",
              " 'albert': 5,\n",
              " 'psychotic': 4,\n",
              " 'prisoner': 8,\n",
              " 'devoted': 6,\n",
              " 'john': 67,\n",
              " 'lennon': 2,\n",
              " 'beatles': 2,\n",
              " 'then': 96,\n",
              " '1974': 1,\n",
              " 'something': 30,\n",
              " 'incredible': 6,\n",
              " '-': 219,\n",
              " 'they': 539,\n",
              " 'fell': 5,\n",
              " 'love': 279,\n",
              " 'deathbed': 1,\n",
              " 'candice': 3,\n",
              " 'klein': 5,\n",
              " 'accidentally': 19,\n",
              " 'asks': 9,\n",
              " 'question': 19,\n",
              " 'did': 16,\n",
              " 'i': 10,\n",
              " 'ever': 41,\n",
              " 'do': 71,\n",
              " 'deserve': 2,\n",
              " '?': 103,\n",
              " 'shot': 27,\n",
              " 'behind-the-scenes': 1,\n",
              " 'look': 32,\n",
              " 'how': 87,\n",
              " 'fictional': 6,\n",
              " 'kung-fu': 5,\n",
              " 'basically': 2,\n",
              " 'making': 34,\n",
              " 'before': 111,\n",
              " 'investigation': 18,\n",
              " 'ends': 19,\n",
              " \"we've\": 1,\n",
              " 'met': 13,\n",
              " 'boyfriends': 4,\n",
              " 'drug': 57,\n",
              " 'dealer': 18,\n",
              " \"alicia's\": 2,\n",
              " \"hadley's\": 1,\n",
              " 'dad': 10,\n",
              " 'nurses': 2,\n",
              " 'doctors': 5,\n",
              " 'orderly': 1,\n",
              " 'exactly': 14,\n",
              " 'good': 60,\n",
              " '&#38': 32,\n",
              " 'evil': 58,\n",
              " 'mexico': 12,\n",
              " '2002': 9,\n",
              " 'based': 55,\n",
              " \"1800's\": 2,\n",
              " \"rainone's\": 1,\n",
              " 'affair': 29,\n",
              " 'singing': 7,\n",
              " 'sensation': 4,\n",
              " 'kelly': 9,\n",
              " 'mcguire': 1,\n",
              " 'whom': 32,\n",
              " 'discovered': 14,\n",
              " 'near': 14,\n",
              " 'demise': 3,\n",
              " 'hands': 28,\n",
              " 'prot&#233': 3,\n",
              " 'g&#233': 3,\n",
              " 'vincent': 12,\n",
              " 'riccola': 1,\n",
              " 'juice': 1,\n",
              " 'fuels': 2,\n",
              " 'roller': 6,\n",
              " 'coaster': 6,\n",
              " 'ride': 14,\n",
              " 'through': 128,\n",
              " 'debauchery-filled': 1,\n",
              " 'decades': 8,\n",
              " 'greed': 8,\n",
              " 'sex': 31,\n",
              " 'drugs': 18,\n",
              " 'roll': 4,\n",
              " 'trapped': 13,\n",
              " 'lovers': 10,\n",
              " 'triangle': 9,\n",
              " 'ruthless': 8,\n",
              " 'game': 32,\n",
              " 'lust': 5,\n",
              " 'betrayal': 10,\n",
              " 'we': 49,\n",
              " 'follow': 21,\n",
              " \"woman's\": 4,\n",
              " 'hypnotic': 1,\n",
              " 'journey': 47,\n",
              " 'discover': 45,\n",
              " 'self': 8,\n",
              " 'decent-but-dull': 1,\n",
              " 'dek': 2,\n",
              " 'loves': 13,\n",
              " 'shirley': 3,\n",
              " 'humiliates': 1,\n",
              " 'proposing': 2,\n",
              " 'warning': 4,\n",
              " 'national': 8,\n",
              " 'since': 37,\n",
              " 'architects': 1,\n",
              " 'either': 6,\n",
              " 'busy': 7,\n",
              " 'otherwise': 4,\n",
              " 'or': 118,\n",
              " 'too': 41,\n",
              " 'conservative': 6,\n",
              " 'style': 5,\n",
              " 'ambivalent': 1,\n",
              " 'honour': 2,\n",
              " 'falls': 67,\n",
              " 'numerobis': 2,\n",
              " 'goes': 63,\n",
              " 'town': 72,\n",
              " 'darkness': 5,\n",
              " \"she's\": 49,\n",
              " 'tooth': 9,\n",
              " 'fairy': 13,\n",
              " 'strange': 36,\n",
              " 'employing': 1,\n",
              " 'home': 110,\n",
              " 'movies': 19,\n",
              " 'newly': 6,\n",
              " 'footage': 16,\n",
              " 'effort': 8,\n",
              " 'expose': 8,\n",
              " 'hungarian': 2,\n",
              " 'mutiple': 1,\n",
              " 'problems': 15,\n",
              " '1940s': 2,\n",
              " 'current': 2,\n",
              " 'matsumoto': 1,\n",
              " 'sawako': 1,\n",
              " 'were': 57,\n",
              " 'happy': 19,\n",
              " 'couple': 36,\n",
              " 'meddling': 2,\n",
              " 'parents': 47,\n",
              " 'chase': 11,\n",
              " 'success': 23,\n",
              " 'push': 5,\n",
              " 'tragic': 15,\n",
              " 'choice': 7,\n",
              " 'elvis': 5,\n",
              " 'teams': 12,\n",
              " 'jack': 35,\n",
              " 'ossie': 1,\n",
              " 'davis': 5,\n",
              " 'fellow': 16,\n",
              " 'nursing': 3,\n",
              " 'resident': 2,\n",
              " 'thinks': 23,\n",
              " 'president': 11,\n",
              " 'f': 1,\n",
              " 'kennedy': 5,\n",
              " 'valiant': 1,\n",
              " 'codgers': 1,\n",
              " 'sally': 8,\n",
              " 'forth': 3,\n",
              " 'battle': 33,\n",
              " 'egyptian': 2,\n",
              " 'entity': 5,\n",
              " 'chosen': 7,\n",
              " 'long-term': 4,\n",
              " 'care': 23,\n",
              " 'facility': 11,\n",
              " 'hunting': 4,\n",
              " 'grounds': 5,\n",
              " 'everywhere': 3,\n",
              " 'plagued': 2,\n",
              " 'cats': 2,\n",
              " 'when': 475,\n",
              " 'meets': 72,\n",
              " 'carol': 8,\n",
              " 'lonely': 14,\n",
              " 'highway': 3,\n",
              " 'must': 129,\n",
              " 'begin': 31,\n",
              " 'avoiding': 5,\n",
              " 'private': 15,\n",
              " 'detective': 35,\n",
              " 'mr': 22,\n",
              " 'barlow': 2,\n",
              " 'terrifying': 7,\n",
              " 'inhuman': 3,\n",
              " 'creature': 14,\n",
              " 'uncover': 5,\n",
              " 'dark': 53,\n",
              " \"charlie's\": 4,\n",
              " 'used': 13,\n",
              " 'living': 64,\n",
              " 'poverty': 5,\n",
              " 'seemed': 2,\n",
              " 'impossible': 6,\n",
              " 'cass': 2,\n",
              " 'cary': 3,\n",
              " 'comfortable': 5,\n",
              " 'bountiful': 1,\n",
              " 'until': 72,\n",
              " 'doqa': 1,\n",
              " 'gracia': 1,\n",
              " 'comes': 81,\n",
              " 'bring': 28,\n",
              " 'normal': 13,\n",
              " 'come': 70,\n",
              " 'dreams': 53,\n",
              " 'those': 42,\n",
              " 'lost': 45,\n",
              " 'possibilities': 6,\n",
              " 'want': 36,\n",
              " 'realize': 33,\n",
              " 'grit': 1,\n",
              " 'determination': 4,\n",
              " 'molly': 3,\n",
              " 'guides': 6,\n",
              " 'epic': 11,\n",
              " 'step': 9,\n",
              " 'ahead': 10,\n",
              " 'authorities': 8,\n",
              " '1': 4,\n",
              " '500': 1,\n",
              " 'miles': 5,\n",
              " \"australia's\": 1,\n",
              " 'outback': 2,\n",
              " 'rabbit-proof': 2,\n",
              " 'fence': 6,\n",
              " 'bisects': 1,\n",
              " 'continent': 2,\n",
              " 'nevertherless': 1,\n",
              " 'feel': 8,\n",
              " 'responsible': 9,\n",
              " 'flag': 4,\n",
              " \"monday's\": 2,\n",
              " 'ceremony': 5,\n",
              " 'doctor': 23,\n",
              " 'realizes': 23,\n",
              " \"it''s\": 1,\n",
              " 'virus': 11,\n",
              " 'advises': 1,\n",
              " 'woo': 1,\n",
              " 'girl': 130,\n",
              " 'somehow': 10,\n",
              " 'realizing': 8,\n",
              " 'munnabhai': 6,\n",
              " 'fallen': 8,\n",
              " 'none': 7,\n",
              " 'younger': 15,\n",
              " 'sister': 45,\n",
              " 'komal': 4,\n",
              " 'can': 130,\n",
              " 'inhabit': 2,\n",
              " 'short': 29,\n",
              " 'time': 171,\n",
              " 'later': 44,\n",
              " 'maine': 5,\n",
              " 'woods': 10,\n",
              " 'overtaken': 1,\n",
              " 'blizzard': 1,\n",
              " 'vicious': 7,\n",
              " 'storm': 8,\n",
              " 'more': 149,\n",
              " 'ominous': 1,\n",
              " 'consequently': 3,\n",
              " 'enthusiastic': 1,\n",
              " 'road': 24,\n",
              " 'roadside': 1,\n",
              " 'obstacles': 6,\n",
              " 'threaten': 7,\n",
              " 'prevent': 15,\n",
              " 'boys': 26,\n",
              " 'competition': 13,\n",
              " 'gets': 71,\n",
              " 'trouble': 22,\n",
              " 'police': 52,\n",
              " 'simon': 16,\n",
              " 'represses': 1,\n",
              " 'death': 95,\n",
              " 'wish': 8,\n",
              " 'decides': 56,\n",
              " 'help': 109,\n",
              " 'chon': 8,\n",
              " 'travels': 16,\n",
              " 'new': 267,\n",
              " 'york': 53,\n",
              " 'roy': 6,\n",
              " \"o'bannon\": 1,\n",
              " 'owen': 4,\n",
              " 'wilson': 6,\n",
              " 'overcome': 11,\n",
              " 'enemy': 15,\n",
              " 'adept': 2,\n",
              " 'technological': 2,\n",
              " 'witchery': 1,\n",
              " 'curse': 9,\n",
              " 'marks': 4,\n",
              " 'destiny': 12,\n",
              " 'becomes': 92,\n",
              " 'less': 10,\n",
              " 'ordinary': 8,\n",
              " 'encounter': 16,\n",
              " 'herb': 3,\n",
              " 'mischievous': 1,\n",
              " 'malevolent': 1,\n",
              " 'geek': 2,\n",
              " 'call': 11,\n",
              " 'themselves': 40,\n",
              " 'd': 7,\n",
              " 'e': 11,\n",
              " 'b': 5,\n",
              " 's': 43,\n",
              " 'starts': 44,\n",
              " 'hakimi': 2,\n",
              " 'freelance': 3,\n",
              " 'scriptwriter': 1,\n",
              " 'send': 6,\n",
              " '7-year-old': 1,\n",
              " 'imelda': 1,\n",
              " \"ex-wife's\": 1,\n",
              " 'stormy': 3,\n",
              " 'night': 69,\n",
              " 'leads': 41,\n",
              " 'informants': 1,\n",
              " 'turn': 43,\n",
              " 'dead': 55,\n",
              " \"nick's\": 5,\n",
              " 'unhappy': 5,\n",
              " \"henry's\": 1,\n",
              " 'protective': 1,\n",
              " \"cop's\": 1,\n",
              " 'second': 24,\n",
              " 'part': 44,\n",
              " 'aki': 1,\n",
              " 'kaurism&#228': 1,\n",
              " \"ki's\": 1,\n",
              " 'finland': 3,\n",
              " 'trilogy': 5,\n",
              " 'follows': 58,\n",
              " 'man': 226,\n",
              " 'arrives': 29,\n",
              " 'helsinki': 1,\n",
              " 'beaten': 4,\n",
              " 'severely': 2,\n",
              " 'develops': 9,\n",
              " 'amnesia': 3,\n",
              " 'accident': 17,\n",
              " 'survivors': 7,\n",
              " 'start': 31,\n",
              " 'dropping': 2,\n",
              " 'flies': 2,\n",
              " 'edgar': 8,\n",
              " 'intent': 4,\n",
              " 'laying': 1,\n",
              " 'down': 99,\n",
              " 'rules': 14,\n",
              " 'turning': 12,\n",
              " 'coddled': 1,\n",
              " 'son': 96,\n",
              " 'someone': 19,\n",
              " 'bound': 5,\n",
              " 'red': 18,\n",
              " 'cord': 1,\n",
              " 'wanders': 4,\n",
              " 'forgotten': 7,\n",
              " 'sudden': 5,\n",
              " 'fame': 17,\n",
              " 'does': 52,\n",
              " 'seem': 20,\n",
              " 'solve': 13,\n",
              " 'everything': 51,\n",
              " 'no': 113,\n",
              " 'option': 3,\n",
              " 'katsuragi': 2,\n",
              " 'use': 22,\n",
              " 'martial': 18,\n",
              " 'arts': 18,\n",
              " 'skills': 9,\n",
              " 'fight': 42,\n",
              " 'muscle': 2,\n",
              " 'dome': 2,\n",
              " 'drawing': 4,\n",
              " 'kids': 38,\n",
              " 'writes': 6,\n",
              " 'children': 37,\n",
              " \"don't\": 20,\n",
              " 'grow': 10,\n",
              " 'artist': 27,\n",
              " \"hasn't\": 10,\n",
              " 'picked': 7,\n",
              " 'brush': 2,\n",
              " 'kidnapped': 15,\n",
              " 'held': 10,\n",
              " 'exchange': 10,\n",
              " 'priceless': 1,\n",
              " 'diamonds': 5,\n",
              " 'leader': 18,\n",
              " 'crew': 23,\n",
              " 'highly': 11,\n",
              " 'skilled': 2,\n",
              " 'urban': 11,\n",
              " 'thieves': 5,\n",
              " 'dmx': 2,\n",
              " 'forges': 1,\n",
              " 'unlikely': 17,\n",
              " 'alliance': 4,\n",
              " 'taiwanese': 1,\n",
              " 'intelligence': 7,\n",
              " 'jet': 2,\n",
              " 'li': 3,\n",
              " 'rescue': 18,\n",
              " 'garmento': 1,\n",
              " 'tells': 52,\n",
              " 'satirical': 1,\n",
              " \"york's\": 7,\n",
              " 'wholesale': 1,\n",
              " 'garment': 1,\n",
              " 'industry': 19,\n",
              " 'shady': 3,\n",
              " 'deals': 8,\n",
              " 'buck': 1,\n",
              " 'ruthlessness': 1,\n",
              " 'prerequisite': 1,\n",
              " 'career': 41,\n",
              " 'rudy': 3,\n",
              " 'yellowshirt': 1,\n",
              " 'investigator': 2,\n",
              " 'department': 10,\n",
              " 'witnesses': 3,\n",
              " 'firsthand': 2,\n",
              " 'painful': 7,\n",
              " 'legacy': 7,\n",
              " 'indian': 11,\n",
              " 'existence': 17,\n",
              " 'journeying': 1,\n",
              " 'vietnam': 7,\n",
              " 'pulaski': 1,\n",
              " 'tennessee': 4,\n",
              " 'danang': 3,\n",
              " 'tensely': 1,\n",
              " 'unfolds': 11,\n",
              " 'cultural': 11,\n",
              " 'differences': 13,\n",
              " 'separation': 3,\n",
              " 'toll': 2,\n",
              " 'riveting': 1,\n",
              " 'longing': 1,\n",
              " 'personal': 34,\n",
              " 'each': 98,\n",
              " 'weekend': 14,\n",
              " 'nothing': 47,\n",
              " 'hangover': 2,\n",
              " 'robert': 25,\n",
              " 'de': 30,\n",
              " 'niro': 4,\n",
              " 'plays': 34,\n",
              " 'therapist': 1,\n",
              " 'obsessive-compulsive': 1,\n",
              " 'agoraphobic': 2,\n",
              " 'left': 57,\n",
              " 'apartment': 14,\n",
              " 'six': 17,\n",
              " 'cyber': 3,\n",
              " 'breakers': 1,\n",
              " 'got': 32,\n",
              " 'dream': 57,\n",
              " 'win': 24,\n",
              " 'dance': 10,\n",
              " 'final': 25,\n",
              " 'reaching': 2,\n",
              " 'usa': 5,\n",
              " 'rival': 6,\n",
              " 'ld': 1,\n",
              " 'deal': 32,\n",
              " 'excessive': 4,\n",
              " 'live': 63,\n",
              " 'passion': 14,\n",
              " 'samia': 1,\n",
              " 'graced': 1,\n",
              " 'instinctive': 1,\n",
              " 'jenny': 13,\n",
              " 'thomas': 20,\n",
              " 'wants': 63,\n",
              " 'become': 98,\n",
              " 'professional': 21,\n",
              " 'dancer': 11,\n",
              " 'genetically': 2,\n",
              " 'engineered': 2,\n",
              " 'immune': 1,\n",
              " \"miles'\": 1,\n",
              " 'charisma': 1,\n",
              " 'draws': 4,\n",
              " 'claire': 11,\n",
              " 'brutally': 8,\n",
              " 'guns': 10,\n",
              " 'harmless': 2,\n",
              " 'dissidents': 2,\n",
              " 'doubt': 6,\n",
              " 'really': 28,\n",
              " 'trust': 12,\n",
              " 'anyone': 18,\n",
              " 'palm': 1,\n",
              " \"springs'\": 1,\n",
              " 'white': 24,\n",
              " 'backdrop': 9,\n",
              " 'fast-paced': 3,\n",
              " 'circuit': 3,\n",
              " 'parties': 8,\n",
              " 'few': 28,\n",
              " 'questionable': 3,\n",
              " 'actions': 11,\n",
              " \"parties'\": 1,\n",
              " 'finally': 48,\n",
              " 'questioning': 3,\n",
              " 'ramu': 2,\n",
              " 'stands': 12,\n",
              " 'repeats': 1,\n",
              " 'wisdom': 7,\n",
              " 'sharonna': 3,\n",
              " ...}"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_vocab(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cye6s152_vfZ"
      },
      "source": [
        "## Split train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AThlKoUCS93V"
      },
      "source": [
        "## <font color='red'>Read code below and answer questions</font>\n",
        "\n",
        "- What is the purpose of `sub_y` and `sub_obj` ? What do they represent here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88YVeGmvEBPi"
      },
      "source": [
        "`sub_content` represents the subjective content, `obj_content` represents objective content.  \n",
        "`sub_y` is initialized as a matrix of zeroes because we want to represent `1 = objective` and `obj_y` is represented as a matrix of ones because we want to represent `0 = subjective`. Here the response variable is binary (1 if it is objecive, 0 if it is not). The subjective and objective content and dependent variables are then concatenated correspondingly to X and Y for training.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16_OhBqx_vfa"
      },
      "outputs": [],
      "source": [
        "sub_content = read_file(PATH/\"quote.tok.gt9.5000\", encoding=\"ISO-8859-1\")\n",
        "obj_content = read_file(PATH/\"plot.tok.gt9.5000\")\n",
        "sub_content = np.array([line.strip() for line in sub_content])\n",
        "obj_content = np.array([line.strip() for line in obj_content])\n",
        "sub_y = np.zeros(len(sub_content))\n",
        "obj_y = np.ones(len(obj_content))\n",
        "X = np.append(sub_content, obj_content)\n",
        "y = np.append(sub_y, obj_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKHGf-SH_vfa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJetsd7G_vfb"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn_SzAh6_vfc",
        "outputId": "4fcb7a4a-3d1b-4abf-ac88-fbd7cae5d070"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array(['will god let her fall or give her a new path ?',\n",
              "        \"the director's twitchy sketchbook style and adroit perspective shifts grow wearisome amid leaden pacing and indifferent craftsmanship ( most notably wretched sound design ) .\",\n",
              "        \"welles groupie/scholar peter bogdanovich took a long time to do it , but he's finally provided his own broadside at publishing giant william randolph hearst .\",\n",
              "        'based on the 1997 john king novel of the same name with a rather odd synopsis : \" a first novel about a seasoned chelsea football club hooligan who represents a disaffected society operating by brutal rules .',\n",
              "        'yet , beneath an upbeat appearance , she is struggling desperately with the emotional and physical scars left by the attack .'],\n",
              "       dtype='<U691'),\n",
              " array([1., 0., 0., 1., 1.]))"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[:5], y_train[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90pMpsB0UuW4",
        "outputId": "50cf7065-8645-4b43-e33b-9be14468d70a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8000,)"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBnet4xzUxIr",
        "outputId": "0dc6a2c9-4e29-4b29-c2d0-5f35d0264d80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['will god let her fall or give her a new path ?',\n",
              "       \"the director's twitchy sketchbook style and adroit perspective shifts grow wearisome amid leaden pacing and indifferent craftsmanship ( most notably wretched sound design ) .\",\n",
              "       \"welles groupie/scholar peter bogdanovich took a long time to do it , but he's finally provided his own broadside at publishing giant william randolph hearst .\",\n",
              "       'based on the 1997 john king novel of the same name with a rather odd synopsis : \" a first novel about a seasoned chelsea football club hooligan who represents a disaffected society operating by brutal rules .',\n",
              "       'yet , beneath an upbeat appearance , she is struggling desperately with the emotional and physical scars left by the attack .'],\n",
              "      dtype='<U691')"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZZmGgZZ_vfc"
      },
      "outputs": [],
      "source": [
        "# getting vocab from training sets\n",
        "data_vocab = get_vocab(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzvEkC8AUQnk"
      },
      "source": [
        "##  <font color='red'>Validate your function get_vocab: (you should see expected result)</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsDEUfcfUgHv"
      },
      "outputs": [],
      "source": [
        "sampletext = X_train[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-unPOqz2_vfc",
        "outputId": "496e8cb7-e117-4d51-904b-592a4bfb374a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Key: 'will' Counts: 1\n",
            "Key: 'god' Counts: 1\n",
            "Key: 'let' Counts: 1\n",
            "Key: 'her' Counts: 3\n",
            "Key: 'fall' Counts: 1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data_vocab0 = get_vocab(sampletext)\n",
        "# test\n",
        "stop = 0\n",
        "for k,v in data_vocab0.items():\n",
        "    print(f\"Key: '{k}'\", f\"Counts: {v}\")\n",
        "    stop += 1\n",
        "    if stop >=5: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnmWortn_vfd"
      },
      "source": [
        "## Embedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-txVsOCpX8sN"
      },
      "source": [
        "- <font color='red'>Note</font> `Embedding` tries to map a text into a vector using neuralnet. You are encouraged to understand as much as possible about embedding from PyTorch document and other tutorials for this project but you are not required to write your own code for it. It's fine to just use the given code here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNyiO3bh_vfd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np_XNcO7_vfe",
        "outputId": "6a5ff18e-3f2e-4325-a4e8-bfc93759a804"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.3628, -0.7968,  0.4457],\n",
              "         [ 0.1522,  1.0493,  0.8779],\n",
              "         [ 1.5134, -0.6654, -0.0792],\n",
              "         [-0.9242, -0.4839, -0.1281],\n",
              "         [-0.3628, -0.7968,  0.4457]]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# an Embedding module containing 10 (words) tensors of size 3\n",
        "embed = nn.Embedding(10, 3)\n",
        "a = torch.LongTensor([[1,2,4,5,1]])\n",
        "embed(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_FbgFdi_vff",
        "outputId": "c8485526-67ec-4e36-d449-86608f856a15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.5324,  0.7288, -0.6350],\n",
              "        [-0.3628, -0.7968,  0.4457],\n",
              "        [ 0.1522,  1.0493,  0.8779],\n",
              "        [ 0.7712, -0.3209, -0.8640],\n",
              "        [ 1.5134, -0.6654, -0.0792],\n",
              "        [-0.9242, -0.4839, -0.1281],\n",
              "        [ 1.3983, -0.5136, -1.5807],\n",
              "        [-2.3788,  0.0872, -0.9536],\n",
              "        [ 0.7670, -0.5708, -0.3470],\n",
              "        [ 0.6567,  1.5418,  0.2362]])"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## here is the randomly initialized embeddings\n",
        "embed.weight.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqMrgFKO_vfh"
      },
      "source": [
        "### Initializing embedding layer with Glove embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57hjwUHp_vfh"
      },
      "source": [
        "To get glove pre-trained embeddings:\n",
        "    `wget http://nlp.stanford.edu/data/glove.6B.zip`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKVP80xeZIdZ"
      },
      "source": [
        "- <font color='red'>Note</font> `glove` is  pre-trained`Embedding` with all weights filled up.  You are encouraged to understand as much as possible about glove embedding from your research, for this project but you are not required to write your own code for it. It's fine just use the given code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx6gOUCjaRkr"
      },
      "source": [
        "- <font color='red'>Complete the function below</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoO-vCBybvlM"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "def unpack_glove():\n",
        "    # download from  http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    url = ('http://nlp.stanford.edu/data/glove.6B.zip')\n",
        "    zip_filepath = os.path.join('/content','glove.6B.zip')\n",
        "    urlretrieve(url, zip_filepath)\n",
        "    # unzip the downloaded file\n",
        "    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
        "      # move files into folder `data`\n",
        "      zip_ref.extractall('/content/data')\n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZt7kjOLcZD0"
      },
      "outputs": [],
      "source": [
        "unpack_glove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l6A4Om7_vfi"
      },
      "source": [
        "In this section we are keeping the whole Glove embeddings. You can decide to keep just the words on your training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyr2jAvvnvtn"
      },
      "source": [
        "\n",
        "### <font color='red'>You must run below:\n",
        "\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbijDtmn_vfi",
        "outputId": "a81f06c8-7809-4a2d-deee-1281cefa7b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
            ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n"
          ]
        }
      ],
      "source": [
        "! head -2 data/glove.6B.50d.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q9aEdI4_vfj"
      },
      "source": [
        "We would like to initialize the embeddings from our model with the pre-trained Glove embeddings. After initializing we should \"freeze\" the embeddings at least initially. The rationale is that we first want the network to learn weights for the other parameters that were randomly initialize. After that phase we could finetune the embeddings to our task.\n",
        "\n",
        "`embed.weight.requires_grad = False` <font color='red'>freezes the embedding parameters (so it cannot be updated during training).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6XOstJw_vfj"
      },
      "source": [
        "The following code initializes the embedding. Here `V` is the vocabulary size and `D` is the embedding size. `pretrained_weight` is a numpy matrix of shape `(V, D)`. Each row is a vector representing each of the V words after embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxBw6SBV_vfk"
      },
      "outputs": [],
      "source": [
        "def loadGloveModel(gloveFile=PATH/\"glove.6B.300d.txt\"):\n",
        "    \"\"\" Loads word vectors into a dictionary.\"\"\"\n",
        "    f = open(gloveFile,'r')\n",
        "    word_vecs = {}\n",
        "    for line in f:\n",
        "        splitLine = line.split()\n",
        "        word = splitLine[0]\n",
        "        word_vecs[word] = np.array([float(val) for val in splitLine[1:]])\n",
        "    return word_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg_4OCqU_vfk"
      },
      "outputs": [],
      "source": [
        "word_vecs = loadGloveModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndEEybyP_vfk",
        "outputId": "0c1ce8ad-b5ba-4396-9ab0-25bf17a3fe40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "400000 21416\n"
          ]
        }
      ],
      "source": [
        "print(len(word_vecs.keys()), len(data_vocab.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9d-Z5nQeaqw"
      },
      "source": [
        "- <font color='red'>Complete the function here</font>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXnIbP19_vfk"
      },
      "outputs": [],
      "source": [
        "def delete_rare_words(word_vecs, data_vocab, min_df=2):\n",
        "    \"\"\" Deletes rare words from data_vocab\n",
        "\n",
        "    Deletes words from data_vocab if they are not in word_vecs\n",
        "    and don't have at least min_df occurrencies in data_vocab.\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    shrink_data_vocab = data_vocab.copy()\n",
        "    for word, word_count in data_vocab.items():\n",
        "      if word_vecs.get(word) is None and word_count < min_df:\n",
        "        del shrink_data_vocab[word]\n",
        "        continue\n",
        "\n",
        "    return shrink_data_vocab # returns shinked data_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4km4OqDE_vfl",
        "outputId": "403feb1b-04ae-4770-ec77-4f07a1012d18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21416"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data_vocab.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_UDwtXW_vfm"
      },
      "outputs": [],
      "source": [
        "# clean up issues here\n",
        "data_vocab = delete_rare_words(word_vecs, data_vocab, min_df=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I23VoMgT_vfm",
        "outputId": "9e21556f-6204-48df-d702-875413ab1494"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18767"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data_vocab.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2rPWPbM_vfn"
      },
      "outputs": [],
      "source": [
        "def create_embedding_matrix(word_vecs, data_vocab, min_df=2, D=300):\n",
        "    \"\"\"Creates embedding matrix from word vectors. \"\"\"\n",
        "    data_vocab = delete_rare_words(word_vecs, data_vocab, min_df)\n",
        "    V = len(data_vocab.keys()) + 2\n",
        "    vocab2index = {}\n",
        "    W = np.zeros((V, D), dtype=\"float32\")\n",
        "    vocab = [\"\", \"UNK\"]\n",
        "    # adding a vector for padding\n",
        "    W[0] = np.zeros(D, dtype='float32')\n",
        "    # adding a vector for rare words\n",
        "    W[1] = np.random.uniform(-0.25, 0.25, D)\n",
        "    vocab2index[\"UNK\"] = 1\n",
        "    i = 2\n",
        "    for word in data_vocab:\n",
        "        if word in word_vecs:\n",
        "            W[i] = word_vecs[word]\n",
        "            vocab2index[word] = i\n",
        "            vocab.append(word)\n",
        "            i += 1\n",
        "        else:\n",
        "            W[i] = np.random.uniform(-0.25,0.25,D)\n",
        "            vocab2index[word] = i\n",
        "            vocab.append(word)\n",
        "            i += 1\n",
        "    return W, np.array(vocab), vocab2index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2oEjoPw_vfn"
      },
      "outputs": [],
      "source": [
        "pretrained_weight, vocab, vocab2index = create_embedding_matrix(word_vecs, data_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww5s5R68_vfn",
        "outputId": "bb19565c-7139-4e72-a8bd-ae484b700afc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18769"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(pretrained_weight) # note that index 0 is for padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O01UK96_vfo",
        "outputId": "f01919c9-7c63-43e3-f34e-e09d9eeedcb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0986,  0.0508, -0.0988,  ..., -0.2125, -0.1108,  0.1963],\n",
              "        [-0.3457,  0.2848, -0.4848,  ..., -0.4811, -0.3120, -0.0681],\n",
              "        ...,\n",
              "        [-0.1369, -0.0570, -0.1921,  ..., -0.2036, -0.4955, -0.2766],\n",
              "        [-0.3170, -0.4958,  0.3020,  ..., -0.1990,  0.0607, -0.1257],\n",
              "        [ 0.2028, -0.3397, -0.1055,  ...,  0.5841, -0.4893,  0.0245]])"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "D = 300\n",
        "V = len(pretrained_weight)\n",
        "emb = nn.Embedding(V, D)\n",
        "emb.weight.data.copy_(torch.from_numpy(pretrained_weight))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPWArgGffoNe"
      },
      "source": [
        "## <font color='red'>Note: </font>\n",
        "\n",
        "So far data_vocab is a dictionary with values representing word frequency in training set while word_vecs is a dictionary with values to be vectors of floats and the vectors are results of training from generic data set--it has nothing to do with training set.\n",
        "\n",
        "### <font color='red'>Questions:\n",
        "\n",
        "- Briefly explain what does the code above do, in particular why 'UNK' is introduced here?\n",
        "- What does it mean?\n",
        "- How many parameters do we have in this embedding matrix?\n",
        "\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJWwGiX4_vfo"
      },
      "source": [
        "\n",
        "## Your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIr-8C8BGEv7"
      },
      "source": [
        "### <font color='red'>Answers:\n",
        "\n",
        "* `create_embedding_matrix` code walkthrough\n",
        "\n",
        "1. delete rare words or words that do not appear in the word vectorizor\n",
        "2. increase V (vocabulary size) by 2\n",
        "3. initialize a dictionary that converts vocabulary to indices inside the W matrix. This will be convenient when we want to obtain the vector embeddings of a particular word using the matrix.\n",
        "4. create W matrix of float weights with size of V by D (vocabulary size + 2 by embedding size)\n",
        "5. Create list `vocab` with 2 initial values of nothing which correspond to padding and **\"UNK\" which stands for unknown words**. These will correspond to the 1st and 2nd row of the W matrix as seen later.\n",
        "6. Assign 1st row of W matrix as vector for padding (initialization =0)\n",
        "7. Assign 2nd row of W matrix as vector for rare words (random initialization between -0.25 and 0.25)\n",
        "8. add key \"UNK\" to vocab2index with value of 1. UNK = unknown\n",
        "9. initialize i as 2 to skip the 1st 2 rows of W (which we have already assigned)\n",
        "10. Loop over each word in reduced `data_vocab` dictionary\n",
        "\n",
        "10a1. If the word is in the `word_vecs` dictionary, then the ith row of the W matrix is assigned to the value of the word vector embedding from `word_vecs`\n",
        "\n",
        "10a2. If the word is not in the `word_vecs` dictionary, this means that there is no embedding for this particular word. Then the ith row of the W matrix is assigned to a randomly initialized vector of corresponding size.\n",
        "\n",
        "\n",
        "10b. The index of the row (`i`) is assigned as a value corresponding to the key (equal to the word) in the `vocab2index` dictionary\n",
        "10c. Append the word to the `vocab` list\n",
        "10d. Increase the counter `i` by 1\n",
        "\n",
        "11. Return weight matrix `W`, vocab list as Numpy array (including \"UNK\" and \"\"), and the dictionary `vocab2index` that converts vocabulary to the row index in matrix W\n",
        "\n",
        "* UNK stands for unknown words, which are words that we do not have embeddings for and therefore require us to creat embeddings for these words.\n",
        "\n",
        "* The total number of parameters is equal to the V x D which is the original vocabulary size (not +2!) multiplied by the size of the vector embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxpHOVR_vfo"
      },
      "source": [
        "## Encoding training and validation sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDOBgrWi_vfo"
      },
      "source": [
        "We will be using 1D Convolutional neural networks as our model. CNNs assume a fixed input size so we need to assume a fixed size and truncate or pad the sentences as needed. Let's find a good value to set our sequence length to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0sY2V1t_vfo"
      },
      "outputs": [],
      "source": [
        "x_len = np.array([len(x.split()) for x in X_train])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtVpwlkV_vfp",
        "outputId": "fc7d9ce3-496d-43f7-b566-e54344d9d6e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "43.0"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.percentile(x_len, 95) # let set the max sequence len to N=40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g9mLJVQ5_vfp",
        "outputId": "bcf89d07-2851-4637-c3a3-7ba25cd72a83"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'will god let her fall or give her a new path ?'"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru6esBJk_vfp",
        "outputId": "0ca47e72-8366-4e62-91b2-6577168cff93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# returns the index of the word or the index of \"UNK\" otherwise\n",
        "vocab2index.get(\"will\", vocab2index[\"UNK\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BycyWf2v_vfp",
        "outputId": "b932521f-5d90-4648-9e83-aed8cf72b0e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 2,  3,  4,  5,  6,  7,  8,  5,  9, 10, 11, 12])"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in X_train[0].split()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mRb-1VV_vfq"
      },
      "outputs": [],
      "source": [
        "def encode_sentence(s, N=40):\n",
        "    enc = np.zeros(N, dtype=np.int32)\n",
        "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()])\n",
        "    l = min(N, len(enc1))\n",
        "    enc[:l] = enc1[:l]\n",
        "    return enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxQnGMYF_vfq",
        "outputId": "42091bcb-5094-4000-8a20-ce3a619501eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 2,  3,  4,  5,  6,  7,  8,  5,  9, 10, 11, 12,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0], dtype=int32)"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encode_sentence(X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3thlPuT_vfr",
        "outputId": "643cdb13-f676-43a8-8881-95b7a057556f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8000, 40)"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train = np.vstack([encode_sentence(x) for x in X_train])\n",
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWHoPb_C_vfr",
        "outputId": "feaafda5-df54-4bcd-84f1-ee0b8f39d8d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2000, 40)"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_val = np.vstack([encode_sentence(x) for x in X_val])\n",
        "x_val.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6PTR0vX_vfs"
      },
      "source": [
        "## Playing and debugging CNN layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWInhuwRhVzw"
      },
      "source": [
        "## <font color='red'>Note: </font>\n",
        "\n",
        "Carefully read the code below and prepare to answer questions at the end of this section\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJMvj_DO_vft"
      },
      "outputs": [],
      "source": [
        "V = len(pretrained_weight)\n",
        "D = 300\n",
        "N = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nam2PeBh_vft",
        "outputId": "59bb06a2-0609-4739-f467-1eec14acf39d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0986,  0.0508, -0.0988,  ..., -0.2125, -0.1108,  0.1963],\n",
              "        [-0.3457,  0.2848, -0.4848,  ..., -0.4811, -0.3120, -0.0681],\n",
              "        ...,\n",
              "        [-0.1369, -0.0570, -0.1921,  ..., -0.2036, -0.4955, -0.2766],\n",
              "        [-0.3170, -0.4958,  0.3020,  ..., -0.1990,  0.0607, -0.1257],\n",
              "        [ 0.2028, -0.3397, -0.1055,  ...,  0.5841, -0.4893,  0.0245]])"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb = nn.Embedding(V, D)\n",
        "emb.weight.data.copy_(torch.from_numpy(pretrained_weight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHdfy-6P_vft",
        "outputId": "0e864beb-6fc0-4f79-e983-5de8990a85cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 40)"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = x_train[:2]\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdllZkZM_vft",
        "outputId": "b47b9985-bbdb-4997-b51d-44f0596f42bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 2,  3,  4,  5,  6,  7,  8,  5,  9, 10, 11, 12,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0],\n",
              "        [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 18, 27, 28, 29,\n",
              "         30, 31, 32, 33, 34, 35, 36,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0]])"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.LongTensor(x)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBTUwXHi_vfu",
        "outputId": "3844429c-8652-4c37-aa13-433d87ac0b9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 40, 300])"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x1 = emb(x)\n",
        "x1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sFhYGay_vfv",
        "outputId": "313942c4-3bca-4fc9-be95-67038e22714d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 300, 40])"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x1 = x1.transpose(1,2)  # needs to convert x to (batch, embedding_dim, sentence_len)\n",
        "x1.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xXrG0tg_vfv"
      },
      "outputs": [],
      "source": [
        "conv_3 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qig0Ac8J_vfv"
      },
      "outputs": [],
      "source": [
        "x3 = conv_3(x1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmRYKHgZ_vfv",
        "outputId": "9a34e8e5-6127-4ad1-81ba-f227edf3cbc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 100, 38])"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x3.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNWDyyIR_vfv"
      },
      "outputs": [],
      "source": [
        "conv_4 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=4)\n",
        "conv_5 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHWOzZML_vfw",
        "outputId": "23909529-7819-4a06-ba56-e49081900a12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 100, 37]) torch.Size([2, 100, 36])\n"
          ]
        }
      ],
      "source": [
        "x4 = conv_4(x1)\n",
        "x5 = conv_5(x1)\n",
        "print(x4.size(), x5.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUCry48x_vfw"
      },
      "source": [
        "Note that the convolution all apply to the same `x1`. How do we combine now the results of the convolutions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0KE71Kn_vfw",
        "outputId": "2ab38d9b-8f23-4909-c080-db74e4269720"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 100, 1])"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 100 3-gram detectors\n",
        "x3 = nn.ReLU()(x3)\n",
        "x3 = nn.MaxPool1d(kernel_size = 38)(x3)\n",
        "x3.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhRX78ua_vfw",
        "outputId": "201076f1-f139-413a-f019-7176963cf3b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 100, 1])"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 100 4-gram detectors\n",
        "x4 = nn.ReLU()(x4)\n",
        "x4 = nn.MaxPool1d(kernel_size = 37)(x4)\n",
        "x4.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4Bhza3k_vfw",
        "outputId": "11bfd68c-3d9e-4013-e744-1faaff706b2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 100, 1])"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 100 5-gram detectors\n",
        "x5 = nn.ReLU()(x5)\n",
        "x5 = nn.MaxPool1d(kernel_size = 36)(x5)\n",
        "x5.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um43Xhjl_vfx",
        "outputId": "07cddf3b-df58-437a-a502-b940f0e39dba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 100, 3])"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# concatenate x3, x4, x5\n",
        "out = torch.cat([x3, x4, x5], 2)\n",
        "out.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZtKeZlu_vfx",
        "outputId": "3c4cad31-5dc5-464c-d5a8-b92f04ae721c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 300])"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = out.view(out.size(0), -1)\n",
        "out.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj4nBZjv_vfx"
      },
      "source": [
        "After this we have a fully connected network. Let's write a network that implements this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv1eJs_L_vfx"
      },
      "source": [
        "## 1D CNN model for sentence classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-vtRrsk_vfx"
      },
      "source": [
        "Notation:\n",
        "* V -- vocabulary size\n",
        "* D -- embedding size\n",
        "* N -- MAX Sentence length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_s8qfYU_vfy"
      },
      "outputs": [],
      "source": [
        "class SentenceCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, V, D, glove_weights):\n",
        "        super(SentenceCNN, self).__init__()\n",
        "        self.glove_weights = glove_weights\n",
        "        self.embedding = nn.Embedding(V, D, padding_idx=0)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(self.glove_weights))\n",
        "        self.embedding.weight.requires_grad = False ## freeze embeddings\n",
        "\n",
        "        self.conv_3 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=3)\n",
        "        self.conv_4 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=4)\n",
        "        self.conv_5 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=5)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.transpose(1,2)\n",
        "        x3 = F.relu(self.conv_3(x))\n",
        "        x4 = F.relu(self.conv_4(x))\n",
        "        x5 = F.relu(self.conv_5(x))\n",
        "        x3 = nn.MaxPool1d(kernel_size = 38)(x3)\n",
        "        x4 = nn.MaxPool1d(kernel_size = 37)(x4)\n",
        "        x5 = nn.MaxPool1d(kernel_size = 36)(x5)\n",
        "        out = torch.cat([x3, x4, x5], 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.dropout(out)\n",
        "        return self.fc(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0XgWpbh_vfy"
      },
      "outputs": [],
      "source": [
        "V = len(pretrained_weight)\n",
        "D = 300\n",
        "N = 40\n",
        "model = SentenceCNN(V, D, glove_weights=pretrained_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SReFRByp_vfy",
        "outputId": "edfa28b7-d2d0-437e-cead-d9d7dcda87eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10, 40)\n"
          ]
        }
      ],
      "source": [
        "# testing the model\n",
        "x = x_train[:10]\n",
        "print(x.shape)\n",
        "x = torch.LongTensor(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Bbp454x_vfy",
        "outputId": "d8862d4a-afcf-43ef-abb1-0646c14648bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 1])"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_hat = model(x)\n",
        "y_hat.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blFEN5-8QwiP",
        "outputId": "ea4aa4db-7daf-472f-f0c8-6e36ed4093e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2956],\n",
            "        [ 0.0108],\n",
            "        [ 0.1466],\n",
            "        [ 0.0215],\n",
            "        [ 0.4906],\n",
            "        [ 0.3931],\n",
            "        [-0.0763],\n",
            "        [ 0.1265],\n",
            "        [-0.0939],\n",
            "        [ 0.2707]], grad_fn=<AddmmBackward0>)\n",
            "torch.Size([10, 1])\n"
          ]
        }
      ],
      "source": [
        "test = model.forward(x)\n",
        "print(test)\n",
        "print(test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ix8zznXhtbz"
      },
      "source": [
        "\n",
        "### <font color='red'>Questions:\n",
        "\n",
        "- What is the output dimension of `.forward()` What does it mean ?\n",
        "- What are parameters to be LEARNED in the model?\n",
        "- in `.forward()`, how are x3, x4, x5 connected ? i.e., are they in a pipeline or in parallel ?\n",
        "- Briefly explain what has been the effect for each of the CNN output x3, x4, x5 ?\n",
        "- How do x3, x4, x5 contribute the prediction ?\n",
        "\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NS8dua7PIcp"
      },
      "source": [
        "### <font color='red'>Answers:\n",
        "\n",
        "* The output dimension of `.forward()` is x by 1. x = number of encoded sentences. x=  10 for the subset above. 1 represents the log loss, which can later be converted to binary output.\n",
        "* The parameters to be LEARNED in the model are the weights in conv_3, conv_4, conv_5 as well as the bias term(s). (The embedding terms are frozen via `requires_grad=False`)\n",
        "* In `.forward()`, x3, x4, and x5 are computed in parallel and stacked vertically (`torch.cat([x3, x4, x5], 2)` concatenates them so that they are on top of each other)\n",
        "* The effect for each of the CNN output x3, x4, x5 is to learn based on 3,4,or 5 grams the pattern for subjective and objective content. In other words, extract features based on different gram lengths. However, since the words may not appear in the same order or there might be words in between the same words, there is additional complexity that a simple neural network cannot account for. However, the use of a convolutional layer allows more complex patterns to be learned. The pooling layer is a subsampling technique. It can remove noise and help significant features to stand out more. For example, some strong words such as \"hate\" might stand out more after pooling.\n",
        "* x3 is a 3 gram detector, x4 is a 4 gram detector, and x5 is a 5 gram detector. What this means is that the model tries to learn the difference between subjective and objective content based on all possible combinations of 3 words, 4 words, and 5 words (corresponding to x3, x4, and x5).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlB068ZS_vfz"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LXaYMTD_vfz"
      },
      "source": [
        "Note that we are not bothering with mini-batches since our dataset is small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWYNYcRf_vf0"
      },
      "outputs": [],
      "source": [
        "model = SentenceCNN(V, D, glove_weights=pretrained_weight) #.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qivZqsrZ_vf0"
      },
      "outputs": [],
      "source": [
        "def val_metrics(m):\n",
        "    model.eval()\n",
        "    x = torch.LongTensor(x_val) #.cuda()\n",
        "    y = torch.Tensor(y_val).unsqueeze(1) #).cuda()\n",
        "    y_hat = m(x)\n",
        "    loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
        "    y_pred = y_hat > 0\n",
        "    correct = (y_pred.float() == y).float().sum()\n",
        "    accuracy = correct/y_pred.shape[0]\n",
        "    return loss.item(), accuracy.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3zaOr8Z_vf0",
        "outputId": "e62c0989-3679-4b14-fce1-fae88a29292b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.6948997974395752, 0.5059999823570251)"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# accuracy of a random model should be around 0.5\n",
        "val_metrics(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dmqsShf_vf0"
      },
      "outputs": [],
      "source": [
        "# this filters parameters with p.requires_grad=True\n",
        "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "optimizer = torch.optim.Adam(parameters, lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMr3JwofjxCJ"
      },
      "source": [
        "\n",
        "### <font color='red'>Questions:\n",
        "\n",
        "- What is the effect of function `parameters` above ?\n",
        "\n",
        "\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Iv3A7llTfpK"
      },
      "source": [
        "### <font color='red'>Answers:\n",
        "\n",
        "The effect of function `parameters()` above is to filter out paramters with `p.requires_grad=True` because we want to update the parameters of the CNN instead of the vector embedding parameters (which we want to fix as aforementioned)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vUqp2C4_vf1"
      },
      "outputs": [],
      "source": [
        "def train_epocs(model, epochs=10, lr=0.01):\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
        "    model.train()\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        x = torch.LongTensor(x_train)  #.cuda()\n",
        "        y = torch.Tensor(y_train).unsqueeze(1)\n",
        "        y_hat = model(x)\n",
        "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        val_loss, accuracy = val_metrics(model)\n",
        "        print(\"train loss %.3f test loss %.3f and accuracy %.3f\" %\n",
        "              (loss.item(), val_loss, accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdZMpWiT_vf1"
      },
      "outputs": [],
      "source": [
        "model = SentenceCNN(V, D, glove_weights=pretrained_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJsaXwfI_vf1",
        "outputId": "f9049681-b690-4147-afd1-b12e1ff79d87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.697 test loss 0.813 and accuracy 0.504\n",
            "train loss 0.776 test loss 0.544 and accuracy 0.681\n",
            "train loss 0.543 test loss 0.533 and accuracy 0.705\n",
            "train loss 0.534 test loss 0.378 and accuracy 0.855\n",
            "train loss 0.368 test loss 0.370 and accuracy 0.849\n",
            "train loss 0.342 test loss 0.419 and accuracy 0.798\n",
            "train loss 0.381 test loss 0.416 and accuracy 0.800\n",
            "train loss 0.373 test loss 0.368 and accuracy 0.839\n",
            "train loss 0.329 test loss 0.327 and accuracy 0.867\n",
            "train loss 0.293 test loss 0.317 and accuracy 0.871\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BQCfrG5_vf2",
        "outputId": "976af8d6-8491-4ac9-8b5f-65f1ce06a114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.Size([100, 300, 3]), torch.Size([100]), torch.Size([100, 300, 4]), torch.Size([100]), torch.Size([100, 300, 5]), torch.Size([100]), torch.Size([1, 300]), torch.Size([1])]\n"
          ]
        }
      ],
      "source": [
        "# how to figure out the parameters\n",
        "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "print([p.size() for p in parameters])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlq0Awdz_vf2"
      },
      "source": [
        "### Unfreezing the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmSB4ePY_vf3"
      },
      "outputs": [],
      "source": [
        "# unfreezing the embeddings\n",
        "model.embedding.weight.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q__lyLi8_vf3",
        "outputId": "50c9776f-78d7-4fe1-dc9e-e70bfe6c6f0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.Size([18769, 300]), torch.Size([100, 300, 3]), torch.Size([100]), torch.Size([100, 300, 4]), torch.Size([100]), torch.Size([100, 300, 5]), torch.Size([100]), torch.Size([1, 300]), torch.Size([1])]\n"
          ]
        }
      ],
      "source": [
        "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "print([p.size() for p in parameters])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UCOyul0ixmA"
      },
      "source": [
        "- Tranin again with embedding unfreezed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9hYNXr1_vf3",
        "outputId": "dbfe630c-d521-4987-d254-3737ea27351c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.289 test loss 0.321 and accuracy 0.867\n",
            "train loss 0.276 test loss 0.312 and accuracy 0.870\n",
            "train loss 0.262 test loss 0.295 and accuracy 0.872\n",
            "train loss 0.246 test loss 0.284 and accuracy 0.877\n",
            "train loss 0.233 test loss 0.277 and accuracy 0.883\n",
            "train loss 0.224 test loss 0.270 and accuracy 0.881\n",
            "train loss 0.212 test loss 0.265 and accuracy 0.885\n",
            "train loss 0.198 test loss 0.263 and accuracy 0.881\n",
            "train loss 0.191 test loss 0.260 and accuracy 0.887\n",
            "train loss 0.182 test loss 0.255 and accuracy 0.887\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8vfuFQr_vf4",
        "outputId": "91a21aff-053c-468c-cbd4-58eef501b88a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.172 test loss 0.252 and accuracy 0.900\n",
            "train loss 0.174 test loss 0.242 and accuracy 0.897\n",
            "train loss 0.154 test loss 0.251 and accuracy 0.895\n",
            "train loss 0.149 test loss 0.254 and accuracy 0.892\n",
            "train loss 0.145 test loss 0.243 and accuracy 0.899\n",
            "train loss 0.134 test loss 0.234 and accuracy 0.904\n",
            "train loss 0.123 test loss 0.230 and accuracy 0.906\n",
            "train loss 0.120 test loss 0.229 and accuracy 0.906\n",
            "train loss 0.116 test loss 0.226 and accuracy 0.909\n",
            "train loss 0.106 test loss 0.225 and accuracy 0.910\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6xTB-JgkZ7T"
      },
      "source": [
        "\n",
        "### <font color='red'>Questions:\n",
        "\n",
        "- In the above we trained before and after unfreezing embeddings. What are the major difference between the two stages ?\n",
        "\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjtC-xc6T_oQ"
      },
      "source": [
        "### <font color='red'>Answers:\n",
        "\n",
        "In the 1st stage, we train the parameters of the CNN layers to learn from the embeddings of the word vector. This prevents us from encountering a problem of a dog chasing a tail (a moving target is not helpful).\n",
        "In the 2nd stage, we allow the parameters of the CNN and the word embeddings to be learned which increases overall accuracy and allows the word embeddings to improve as well.\n",
        "\n",
        "I quote the above description to help explain:\n",
        "\n",
        "`The rationale is that we first want the network to learn weights for the other parameters that were randomly initialized. After that phase we could finetune the embeddings to our task.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn2B3va4_vf4"
      },
      "source": [
        "## Whithout pretrain emmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b7u9vTwkw9Y"
      },
      "source": [
        "\n",
        "### <font color='red'>Your turn: </font>\n",
        "\n",
        "\n",
        "- Complete the `SentenceCNN2` below for `__init__` method, but unlike the class `SentenceCNN` -- you <font color='red'> do not </font> fill in pretrained weights for the embedding. Then proceed to next stage for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeNJYwZv_vf4"
      },
      "outputs": [],
      "source": [
        "class SentenceCNN2(nn.Module):\n",
        "\n",
        "    def __init__(self, V, D):\n",
        "        # your code here\n",
        "        super(SentenceCNN2, self).__init__()\n",
        "\n",
        "        # Create an empty matrix (all zeros) of the desired shape\n",
        "        self.random_weights = np.zeros((V, D), dtype=\"float32\")\n",
        "        for i in range(V):\n",
        "          self.random_weights[i] = np.random.uniform(-0.25, 0.25, D)\n",
        "\n",
        "        self.embedding = nn.Embedding(V, D, padding_idx=0)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(self.random_weights))\n",
        "        self.embedding.weight.requires_grad = True ## unfreeze embeddings\n",
        "\n",
        "        self.conv_3 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=3)\n",
        "        self.conv_4 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=4)\n",
        "        self.conv_5 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=5)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.transpose(1,2)\n",
        "        x3 = F.relu(self.conv_3(x))\n",
        "        x4 = F.relu(self.conv_4(x))\n",
        "        x5 = F.relu(self.conv_5(x))\n",
        "        x3 = nn.MaxPool1d(kernel_size = 38)(x3)\n",
        "        x4 = nn.MaxPool1d(kernel_size = 37)(x4)\n",
        "        x5 = nn.MaxPool1d(kernel_size = 36)(x5)\n",
        "        out = torch.cat([x3, x4, x5], 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.dropout(out)\n",
        "        return self.fc(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMI8nUuU_vf4"
      },
      "outputs": [],
      "source": [
        "V = len(pretrained_weight)\n",
        "model = SentenceCNN2(V, D=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o45f-6Fq_vf5",
        "outputId": "ed58b046-2729-468f-892e-f608cac9a8f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.696 test loss 0.669 and accuracy 0.527\n",
            "train loss 0.663 test loss 0.725 and accuracy 0.532\n",
            "train loss 0.717 test loss 0.573 and accuracy 0.791\n",
            "train loss 0.550 test loss 0.578 and accuracy 0.648\n",
            "train loss 0.535 test loss 0.505 and accuracy 0.753\n",
            "train loss 0.442 test loss 0.400 and accuracy 0.860\n",
            "train loss 0.323 test loss 0.356 and accuracy 0.850\n",
            "train loss 0.256 test loss 0.329 and accuracy 0.857\n",
            "train loss 0.202 test loss 0.281 and accuracy 0.882\n",
            "train loss 0.137 test loss 0.265 and accuracy 0.883\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-7razty_vf5",
        "outputId": "b1520cd3-e72b-4cd8-9523-7ad41e0a7843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.096 test loss 0.259 and accuracy 0.892\n",
            "train loss 0.090 test loss 0.257 and accuracy 0.892\n",
            "train loss 0.085 test loss 0.255 and accuracy 0.891\n",
            "train loss 0.079 test loss 0.253 and accuracy 0.893\n",
            "train loss 0.073 test loss 0.252 and accuracy 0.891\n",
            "train loss 0.069 test loss 0.252 and accuracy 0.892\n",
            "train loss 0.064 test loss 0.251 and accuracy 0.894\n",
            "train loss 0.059 test loss 0.251 and accuracy 0.895\n",
            "train loss 0.056 test loss 0.250 and accuracy 0.896\n",
            "train loss 0.052 test loss 0.250 and accuracy 0.897\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SQuWNnu_vf5",
        "outputId": "e3c7afc4-c729-4beb-c5d6-96e8b88aea22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.048 test loss 0.250 and accuracy 0.896\n",
            "train loss 0.044 test loss 0.251 and accuracy 0.897\n",
            "train loss 0.040 test loss 0.252 and accuracy 0.897\n",
            "train loss 0.038 test loss 0.253 and accuracy 0.899\n",
            "train loss 0.034 test loss 0.255 and accuracy 0.897\n",
            "train loss 0.031 test loss 0.256 and accuracy 0.897\n",
            "train loss 0.028 test loss 0.259 and accuracy 0.895\n",
            "train loss 0.026 test loss 0.261 and accuracy 0.895\n",
            "train loss 0.023 test loss 0.263 and accuracy 0.893\n",
            "train loss 0.021 test loss 0.266 and accuracy 0.896\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzKaFjrC_vf5",
        "outputId": "29e8ea87-6a04-4109-c179-76539d71d27f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.020 test loss 0.268 and accuracy 0.897\n",
            "train loss 0.017 test loss 0.271 and accuracy 0.896\n",
            "train loss 0.016 test loss 0.274 and accuracy 0.896\n",
            "train loss 0.014 test loss 0.277 and accuracy 0.896\n",
            "train loss 0.013 test loss 0.281 and accuracy 0.895\n",
            "train loss 0.011 test loss 0.284 and accuracy 0.896\n",
            "train loss 0.010 test loss 0.288 and accuracy 0.896\n",
            "train loss 0.009 test loss 0.292 and accuracy 0.896\n",
            "train loss 0.008 test loss 0.297 and accuracy 0.896\n",
            "train loss 0.007 test loss 0.301 and accuracy 0.896\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJLLP7aN_vf5",
        "outputId": "15e1adfc-82b7-4e37-9923-43f3995c8c7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.007 test loss 0.305 and accuracy 0.895\n",
            "train loss 0.005 test loss 0.310 and accuracy 0.895\n",
            "train loss 0.005 test loss 0.315 and accuracy 0.895\n",
            "train loss 0.004 test loss 0.321 and accuracy 0.895\n",
            "train loss 0.004 test loss 0.326 and accuracy 0.894\n",
            "train loss 0.003 test loss 0.331 and accuracy 0.892\n",
            "train loss 0.003 test loss 0.337 and accuracy 0.894\n",
            "train loss 0.003 test loss 0.342 and accuracy 0.894\n",
            "train loss 0.002 test loss 0.347 and accuracy 0.892\n",
            "train loss 0.002 test loss 0.353 and accuracy 0.892\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFllJ_damBBc"
      },
      "source": [
        "\n",
        "### <font color='red'>Questions:\n",
        "\n",
        "- In the above we trained the model without pretrained weights for embedding, how is the model performance compared to previous one with pretrained weights?\n",
        "- Briefly Explain why ?\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujR9KSwaNUgT"
      },
      "source": [
        "### <font color='red'>Answers:\n",
        "\n",
        "* The model performance by the end of the 1st 10 epochs is much better than the one with pretrained weights for embedding (last testing loss is 0.24 instead of 0.428). Although the first 10 epochs for pretrained weights is 0.005 and the one without pretrained weights is 0.01. The lowest testing loss for the model with pretrained weights for embedding is 0.225 and the lowest testing loss for the model above without pretrained weights for embedding is 0.250, which is worse. In fact, in the last 10 epochs of `Sentence2CNN`, the testing loss increased, indictating overfitting.\n",
        "* My explanation is that randomized embeddings are noisy and it is hard to find the patterns unless we use some type of regularization to prevent overfitting. The glove embeddings are general but good enough to at least help find a rough sense of the real features. It is also not an apples to apples comparision because we used different learning rates and different number of epochs (I didn't change the code above because the instructions said not to)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F420fj-Dmn_N"
      },
      "source": [
        "\n",
        "### <font color='red'>Your turn:\n",
        "\n",
        "- Try to write a third model with pre-trained weights for embedding, but you want to improve perfornace by using more CNN layer(s). Demonstrate your code and show the model performance. In comparison, please use the same number of epoch and learning rates.\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWWGb93qNMK8"
      },
      "source": [
        "## 8 Layer CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNCcx2gJAp3o"
      },
      "outputs": [],
      "source": [
        "class SentenceCNN3_frozen(nn.Module):\n",
        "\n",
        "    def __init__(self, V, D, glove_weights):\n",
        "        # your code here\n",
        "        super(SentenceCNN3_frozen, self).__init__()\n",
        "\n",
        "        self.glove_weights = glove_weights\n",
        "        self.embedding = nn.Embedding(V, D, padding_idx=0)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(self.glove_weights))\n",
        "        self.embedding.weight.requires_grad = False ## freeze embeddings\n",
        "\n",
        "        self.conv_3 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=3)\n",
        "        self.conv_4 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=4)\n",
        "        self.conv_5 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=5)\n",
        "        self.conv_6 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=6)\n",
        "        self.conv_7 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=7)\n",
        "        self.conv_8 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=8)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc = nn.Linear(600, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.transpose(1,2)\n",
        "        x3 = F.relu(self.conv_3(x))\n",
        "        x4 = F.relu(self.conv_4(x))\n",
        "        x5 = F.relu(self.conv_5(x))\n",
        "        x6 = F.relu(self.conv_6(x))\n",
        "        x7 = F.relu(self.conv_7(x))\n",
        "        x8 = F.relu(self.conv_8(x))\n",
        "        x3 = nn.MaxPool1d(kernel_size = 38)(x3)\n",
        "        x4 = nn.MaxPool1d(kernel_size = 37)(x4)\n",
        "        x5 = nn.MaxPool1d(kernel_size = 36)(x5)\n",
        "        x6 = nn.MaxPool1d(kernel_size = 35)(x6)\n",
        "        x7 = nn.MaxPool1d(kernel_size = 34)(x7)\n",
        "        x8 = nn.MaxPool1d(kernel_size = 33)(x8)\n",
        "        out = torch.cat([x3, x4, x5, x6, x7, x8], 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.dropout(out)\n",
        "        return self.fc(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3ky8SWzAp3y"
      },
      "outputs": [],
      "source": [
        "V = len(pretrained_weight)\n",
        "model = SentenceCNN3_frozen(V, D=300, glove_weights=pretrained_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A_RBzm3Z2gL",
        "outputId": "4108dbca-54bd-4196-95fc-5b9470e5e6a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.700 test loss 0.657 and accuracy 0.506\n",
            "train loss 0.643 test loss 0.538 and accuracy 0.850\n",
            "train loss 0.528 test loss 0.516 and accuracy 0.755\n",
            "train loss 0.506 test loss 0.451 and accuracy 0.850\n",
            "train loss 0.437 test loss 0.414 and accuracy 0.871\n",
            "train loss 0.392 test loss 0.405 and accuracy 0.840\n",
            "train loss 0.376 test loss 0.382 and accuracy 0.849\n",
            "train loss 0.352 test loss 0.348 and accuracy 0.874\n",
            "train loss 0.321 test loss 0.328 and accuracy 0.877\n",
            "train loss 0.306 test loss 0.320 and accuracy 0.878\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbi-Ywj6Ap3y",
        "outputId": "cfe0553a-606e-4b7a-9fd7-28912bb6ceb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.300 test loss 8.015 and accuracy 0.494\n",
            "train loss 7.887 test loss 1.976 and accuracy 0.494\n",
            "train loss 1.909 test loss 0.519 and accuracy 0.692\n",
            "train loss 0.501 test loss 0.717 and accuracy 0.508\n",
            "train loss 0.736 test loss 0.666 and accuracy 0.507\n",
            "train loss 0.682 test loss 0.605 and accuracy 0.510\n",
            "train loss 0.603 test loss 0.587 and accuracy 0.515\n",
            "train loss 0.575 test loss 0.588 and accuracy 0.527\n",
            "train loss 0.573 test loss 0.590 and accuracy 0.561\n",
            "train loss 0.573 test loss 0.582 and accuracy 0.585\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDn_DAgFAp3z",
        "outputId": "d29dcaf7-2ec5-4ba1-b963-fde5959ea701"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.561 test loss 0.570 and accuracy 0.596\n",
            "train loss 0.546 test loss 0.560 and accuracy 0.609\n",
            "train loss 0.532 test loss 0.550 and accuracy 0.619\n",
            "train loss 0.520 test loss 0.541 and accuracy 0.632\n",
            "train loss 0.511 test loss 0.533 and accuracy 0.647\n",
            "train loss 0.503 test loss 0.527 and accuracy 0.660\n",
            "train loss 0.495 test loss 0.521 and accuracy 0.677\n",
            "train loss 0.488 test loss 0.514 and accuracy 0.695\n",
            "train loss 0.481 test loss 0.507 and accuracy 0.715\n",
            "train loss 0.472 test loss 0.499 and accuracy 0.737\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmqXxLKnAp3z",
        "outputId": "dd25922d-43b0-4260-d5e6-2bb6c79be5c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.462 test loss 0.490 and accuracy 0.769\n",
            "train loss 0.451 test loss 0.483 and accuracy 0.789\n",
            "train loss 0.441 test loss 0.474 and accuracy 0.836\n",
            "train loss 0.432 test loss 0.465 and accuracy 0.844\n",
            "train loss 0.422 test loss 0.454 and accuracy 0.851\n",
            "train loss 0.410 test loss 0.444 and accuracy 0.855\n",
            "train loss 0.398 test loss 0.433 and accuracy 0.858\n",
            "train loss 0.386 test loss 0.423 and accuracy 0.856\n",
            "train loss 0.375 test loss 0.412 and accuracy 0.858\n",
            "train loss 0.364 test loss 0.403 and accuracy 0.861\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ntdZWNnAp3z",
        "outputId": "65cbbd34-3584-457e-9e33-b1730b2839e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.351 test loss 0.394 and accuracy 0.866\n",
            "train loss 0.342 test loss 0.386 and accuracy 0.864\n",
            "train loss 0.332 test loss 0.376 and accuracy 0.863\n",
            "train loss 0.319 test loss 0.367 and accuracy 0.863\n",
            "train loss 0.308 test loss 0.358 and accuracy 0.864\n",
            "train loss 0.298 test loss 0.350 and accuracy 0.865\n",
            "train loss 0.288 test loss 0.343 and accuracy 0.867\n",
            "train loss 0.278 test loss 0.337 and accuracy 0.868\n",
            "train loss 0.269 test loss 0.331 and accuracy 0.868\n",
            "train loss 0.260 test loss 0.327 and accuracy 0.869\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "phpuqcI8Ap3z",
        "outputId": "cf341681-a3e4-4a6c-cb48-0e933beaf653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.254 test loss 0.322 and accuracy 0.873\n",
            "train loss 0.247 test loss 0.318 and accuracy 0.875\n",
            "train loss 0.240 test loss 0.314 and accuracy 0.876\n",
            "train loss 0.233 test loss 0.312 and accuracy 0.877\n",
            "train loss 0.227 test loss 0.308 and accuracy 0.877\n",
            "train loss 0.220 test loss 0.305 and accuracy 0.877\n",
            "train loss 0.215 test loss 0.303 and accuracy 0.878\n",
            "train loss 0.209 test loss 0.301 and accuracy 0.877\n",
            "train loss 0.204 test loss 0.299 and accuracy 0.878\n",
            "train loss 0.198 test loss 0.297 and accuracy 0.879\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5leQ1xfZAz-"
      },
      "source": [
        "### <font color='red'>Answers:\n",
        "\n",
        "The lowest testing loss is worse than the 3-layer NN without pretrain embeddings (0.297 > 0.250) and worse than the 3 layer NN with pretrain embeddings (0.297 > 0.250). I suspect that 6 layers might be too deep and therefore more iterations and epochs are required to find the true patterns (I don't think it is overfitting yet because the loss is still decreasing slowly).\n",
        "some overfitting is happening when I train for that many epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZTRUwqDNRQC"
      },
      "source": [
        "## 8 Layer CNN with non-Frozen Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iePdCswEhg4z"
      },
      "outputs": [],
      "source": [
        "class SentenceCNN3(nn.Module):\n",
        "\n",
        "    def __init__(self, V, D, glove_weights):\n",
        "        # your code here\n",
        "        super(SentenceCNN3, self).__init__()\n",
        "\n",
        "        self.glove_weights = glove_weights\n",
        "        self.embedding = nn.Embedding(V, D, padding_idx=0)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(self.glove_weights))\n",
        "        self.embedding.weight.requires_grad = True ## don't freeze embeddings\n",
        "\n",
        "        self.conv_3 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=3)\n",
        "        self.conv_4 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=4)\n",
        "        self.conv_5 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=5)\n",
        "        self.conv_6 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=6)\n",
        "        self.conv_7 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=7)\n",
        "        self.conv_8 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=8)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc = nn.Linear(600, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.transpose(1,2)\n",
        "        x3 = F.relu(self.conv_3(x))\n",
        "        x4 = F.relu(self.conv_4(x))\n",
        "        x5 = F.relu(self.conv_5(x))\n",
        "        x6 = F.relu(self.conv_6(x))\n",
        "        x7 = F.relu(self.conv_7(x))\n",
        "        x8 = F.relu(self.conv_8(x))\n",
        "        x3 = nn.MaxPool1d(kernel_size = 38)(x3)\n",
        "        x4 = nn.MaxPool1d(kernel_size = 37)(x4)\n",
        "        x5 = nn.MaxPool1d(kernel_size = 36)(x5)\n",
        "        x6 = nn.MaxPool1d(kernel_size = 35)(x6)\n",
        "        x7 = nn.MaxPool1d(kernel_size = 34)(x7)\n",
        "        x8 = nn.MaxPool1d(kernel_size = 33)(x8)\n",
        "        out = torch.cat([x3, x4, x5, x6, x7, x8], 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.dropout(out)\n",
        "        return self.fc(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4v6vLOCohg40"
      },
      "outputs": [],
      "source": [
        "V = len(pretrained_weight)\n",
        "model = SentenceCNN3(V, D=300, glove_weights=pretrained_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Yx7yUnzThg40",
        "outputId": "fac99454-aa98-4400-84cc-6aa171a3ea2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.703 test loss 3.721 and accuracy 0.506\n",
            "train loss 3.766 test loss 0.621 and accuracy 0.711\n",
            "train loss 0.540 test loss 0.740 and accuracy 0.632\n",
            "train loss 0.644 test loss 0.435 and accuracy 0.804\n",
            "train loss 0.366 test loss 0.389 and accuracy 0.876\n",
            "train loss 0.323 test loss 0.410 and accuracy 0.839\n",
            "train loss 0.336 test loss 0.421 and accuracy 0.816\n",
            "train loss 0.334 test loss 0.405 and accuracy 0.825\n",
            "train loss 0.301 test loss 0.365 and accuracy 0.851\n",
            "train loss 0.245 test loss 0.317 and accuracy 0.876\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DT-XR3Nmhg40",
        "outputId": "50247b6f-82d5-409b-a60f-af8a8051a77c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.182 test loss 0.297 and accuracy 0.885\n",
            "train loss 0.159 test loss 0.282 and accuracy 0.896\n",
            "train loss 0.139 test loss 0.271 and accuracy 0.899\n",
            "train loss 0.122 test loss 0.264 and accuracy 0.900\n",
            "train loss 0.107 test loss 0.260 and accuracy 0.905\n",
            "train loss 0.098 test loss 0.259 and accuracy 0.903\n",
            "train loss 0.087 test loss 0.260 and accuracy 0.902\n",
            "train loss 0.081 test loss 0.262 and accuracy 0.904\n",
            "train loss 0.074 test loss 0.265 and accuracy 0.905\n",
            "train loss 0.069 test loss 0.267 and accuracy 0.905\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLe_Sn5Ehg40",
        "outputId": "4d918d5c-5741-4fa2-94ab-3f793b3f062d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.063 test loss 0.264 and accuracy 0.908\n",
            "train loss 0.058 test loss 0.264 and accuracy 0.908\n",
            "train loss 0.052 test loss 0.265 and accuracy 0.908\n",
            "train loss 0.047 test loss 0.268 and accuracy 0.908\n",
            "train loss 0.043 test loss 0.270 and accuracy 0.906\n",
            "train loss 0.038 test loss 0.272 and accuracy 0.908\n",
            "train loss 0.034 test loss 0.275 and accuracy 0.908\n",
            "train loss 0.030 test loss 0.278 and accuracy 0.907\n",
            "train loss 0.027 test loss 0.282 and accuracy 0.908\n",
            "train loss 0.024 test loss 0.286 and accuracy 0.908\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JShaBnzBhg40",
        "outputId": "0443916d-0dc8-450e-dd43-a0d8b491e9e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.021 test loss 0.293 and accuracy 0.909\n",
            "train loss 0.018 test loss 0.298 and accuracy 0.908\n",
            "train loss 0.016 test loss 0.304 and accuracy 0.910\n",
            "train loss 0.014 test loss 0.310 and accuracy 0.911\n",
            "train loss 0.012 test loss 0.317 and accuracy 0.911\n",
            "train loss 0.010 test loss 0.325 and accuracy 0.913\n",
            "train loss 0.009 test loss 0.334 and accuracy 0.914\n",
            "train loss 0.007 test loss 0.341 and accuracy 0.915\n",
            "train loss 0.006 test loss 0.349 and accuracy 0.915\n",
            "train loss 0.005 test loss 0.357 and accuracy 0.914\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcawROHFhg40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "199823f9-535f-47a0-86b8-761c1fa2f852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss 0.004 test loss 0.371 and accuracy 0.914\n",
            "train loss 0.004 test loss 0.376 and accuracy 0.914\n",
            "train loss 0.003 test loss 0.385 and accuracy 0.914\n",
            "train loss 0.002 test loss 0.395 and accuracy 0.915\n",
            "train loss 0.002 test loss 0.405 and accuracy 0.914\n",
            "train loss 0.002 test loss 0.416 and accuracy 0.914\n",
            "train loss 0.001 test loss 0.426 and accuracy 0.914\n",
            "train loss 0.001 test loss 0.435 and accuracy 0.914\n",
            "train loss 0.001 test loss 0.443 and accuracy 0.914\n",
            "train loss 0.001 test loss 0.452 and accuracy 0.914\n"
          ]
        }
      ],
      "source": [
        "train_epocs(model, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q2CJd6IdgR6"
      },
      "source": [
        "### <font color='red'>Answers:\n",
        "\n",
        "In general the performance is worse than the one with frozen Glove embeddings. However the comparing their lowest testing loss it's 0.259 < 0. 297, so it may seem like using non-frozen embeddings is a good choice. However, the performance becomes so bad towards the end of the epochs that the testing loss goes up and overfitting happens.\n",
        "\n",
        "I think it's because there's too many parameters to learn, so if we don't fix some of them it will cause the NN to either require a lot more epochs and iterations to converge to a low testing loss or require regularization to prevent overfitting.\n",
        "\n",
        "We can make the inferred conclusion that when there are lots of parameters to be learned it is good to freeze some of them with pretrained embeddings. However, when there are only few parameters to be learned it is better to unfreeze all of them and start from scratch."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}